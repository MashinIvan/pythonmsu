{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### requests & bs4 101\n",
    "\n",
    "__Ducomentations__:\n",
    " - [requests](https://requests.readthedocs.io/en/master/)\n",
    " - [bs4](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    " \n",
    "__Installation__:\n",
    "> pip install requests\n",
    "\n",
    "> pip install bs4\n",
    "\n",
    "or combined\n",
    "> pip install requests, bs4\n",
    " \n",
    "__Quick start__:\n",
    "```python\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "resp = requests.get('https://www.imdb.com/chart/top/')\n",
    "soup = bs(resp.text)\n",
    "\n",
    "films_list = soup.find('tbody', {'class': 'lister-list'})\n",
    "films = films_list.find_all('tr')\n",
    "```\n",
    "\n",
    "__BeautifulSoup object main methods__:\n",
    "```python\n",
    "soup.find(tag, {attr: value})\n",
    "soup.find_all(tag, {attr: value})\n",
    "```\n",
    "basically that is it\n",
    "\n",
    "for more BeautifulSoup object methods see\n",
    "```python\n",
    "dir(soup)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rank': 1, 'name': 'Побег из Шоушенка', 'year': 1994, 'rating': 9.2}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "resp = requests.get('https://www.imdb.com/chart/top/')\n",
    "soup = bs(resp.text)\n",
    "\n",
    "films_set = soup.find('tbody', {'class': 'lister-list'}).find_all('tr')\n",
    "films = [{\n",
    "    'rank': int(\n",
    "        film.find_all('td')[1].text.split('\\n')[1].strip('.').strip()\n",
    "    ),\n",
    "    'name': film.find_all('td')[1].text.split('\\n')[2].strip(),\n",
    "    'year': int(\n",
    "        film.find_all('td')[1].text.split('\\n')[3].strip('()')\n",
    "    ),\n",
    "    'rating': float(\n",
    "        film.find_all('td')[2].text.strip()\n",
    "    )\n",
    " } for film in films_set]\n",
    "\n",
    "films[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rank': 101, 'name': 'Малыш', 'year': 1921, 'rating': 8.2}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# oldest film\n",
    "sorted(films, key=lambda x: x['year'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rank': 198, 'name': 'Шерлок младший', 'year': 1924, 'rating': 8.1}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# newest film\n",
    "sorted(films, key=lambda x: x['year'])[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1988.14, 1988.02, 1987.66, 1987.76, 1987.18]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# average age of films by bins of 50\n",
    "film_groups = [films[i:i+50] for i in range(len(films)//50)]\n",
    "average_by_group = []\n",
    "for group in film_groups:\n",
    "    years = [film['year'] for film in group]\n",
    "    average_by_group.append(\n",
    "        sum(years)/len(years))\n",
    "    \n",
    "average_by_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rank': 1,\n",
       " 'name': 'Побег из Шоушенка',\n",
       " 'year': 1994,\n",
       " 'rating': 9.2,\n",
       " 'url': 'https://imdb.com/title/tt0111161/'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i, film in enumerate(films_set):\n",
    "    url = 'https://imdb.com' + film.find('td').find('a').attrs['href']\n",
    "    films[i].update({'url': url})\n",
    "    \n",
    "films[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Drama'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try for one\n",
    "i = 0\n",
    "film = films[i]\n",
    "\n",
    "resp = requests.get(film['url'])\n",
    "soup = bs(resp.text)\n",
    "\n",
    "genre = soup.find('div', {'class': 'subtext'}).find('a').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extrapolate for all\n",
    "for i, film in enumerate(films):\n",
    "    resp = requests.get(film['url'])\n",
    "    soup = bs(resp.text)\n",
    "\n",
    "    genre = soup.find('div', {'class': 'subtext'}).find('a').text\n",
    "    film.update({'genre': genre})\n",
    "    \n",
    "    # takes too long, whant to know the progress\n",
    "    # so break here, tweak it below\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Films scraped: 1\r"
     ]
    }
   ],
   "source": [
    "# a little tweak \n",
    "for i, film in enumerate(films):\n",
    "    resp = requests.get(film['url'])\n",
    "    soup = bs(resp.text)\n",
    "\n",
    "    genre = soup.find('div', {'class': 'subtext'}).find('a').text\n",
    "    film.update({'genre': genre})\n",
    "    \n",
    "    print('Films scraped:', i+1, end='\\r')\n",
    "    \n",
    "    # takes too long\n",
    "    # tweak it even more below to find why\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1604632426.52467"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting response time: 1.8876850605010986\n",
      "creating soup time: 0.16862916946411133\n",
      "finding genre time: 0.004058122634887695\n"
     ]
    }
   ],
   "source": [
    "# more tweaks\n",
    "for i, film in enumerate(films):\n",
    "    t = time.time()\n",
    "    resp = requests.get(film['url'])\n",
    "    print('getting response time:', time.time() - t)\n",
    "    \n",
    "    t = time.time()\n",
    "    soup = bs(resp.text)\n",
    "    print('creating soup time:', time.time() - t)\n",
    "    \n",
    "    t = time.time()    \n",
    "    genre = soup.find('div', {'class': 'subtext'}).find('a').text\n",
    "    film.update({'genre': genre})\n",
    "    print('finding genre time:', time.time() - t)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so the worst thing here is getting response from the server\n",
    "# nothing special to do about it, so leave it as it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Films scraped: 1\r"
     ]
    }
   ],
   "source": [
    "for i, film in enumerate(films):\n",
    "    resp = requests.get(film['url'])\n",
    "    soup = bs(resp.text)\n",
    "\n",
    "    genre = soup.find('div', {'class': 'subtext'}).find('a').text\n",
    "    film.update({'genre': genre})\n",
    "    \n",
    "    print('Films scraped:', i+1, end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = requests.get('https://ru.wikipedia.org/wiki/%D0%93%D0%BE%D0%B3%D0%B8%D1%8F,_%D0%95%D0%BB%D0%B5%D0%BD%D0%B0_%D0%9E%D1%82%D0%B0%D1%80%D0%B8%D0%B5%D0%B2%D0%BD%D0%B0')\n",
    "soup = bs(resp.text)\n",
    "\n",
    "a_tags = soup.find_all('a')\n",
    "links = []\n",
    "for tag in a_tags:\n",
    "    if 'href' in tag.attrs:\n",
    "        link = tag.attrs['href']\n",
    "\n",
    "        links.append(tag.attrs['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_wiki_links(url):\n",
    "    resp = requests.get(url)\n",
    "    if not resp.status_code == 200:\n",
    "        return 'error accessing website'\n",
    "    soup = bs(resp.text)\n",
    "    \n",
    "    a_tags = soup.find_all('a')\n",
    "    links = []\n",
    "    for tag in a_tags:\n",
    "        if 'href' in tag.attrs:\n",
    "            link = tag.attrs['href']\n",
    "            if link.startswith('/wiki/'):\n",
    "                links.append(tag.attrs['href'])\n",
    "    \n",
    "    return links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus\n",
    "__working with urls__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need a library called `urllib`\n",
    "\n",
    "It should be preinstalled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse, parse_qs, urlunparse, urlencode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParseResult(scheme='https', netloc='cacs.econ.msu.ru', path='/index.php', params='', query='mnu=75?&selst=20717', fragment='')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://cacs.econ.msu.ru/index.php?mnu=75?&selst=20717'\n",
    "parse = urlparse(url)\n",
    "\n",
    "parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mnu': ['75?'], 'selst': ['20717']}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parse query of url into python dictionary\n",
    "parse_qs(parse.query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mnu=75&selst=20716'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unparse a dictionary into query\n",
    "my_query = {'mnu': '75', 'selst': '20716'}\n",
    "urlencode(my_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://cacs.econ.msu.ru/index.php?mnu=75&selst=20717'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build custom url based on params\n",
    "my_id = '20717'\n",
    "\n",
    "urlunparse([\n",
    "    'https',\n",
    "    'cacs.econ.msu.ru',\n",
    "    '/index.php',\n",
    "    '',\n",
    "    urlencode({\n",
    "        'mnu': '75',\n",
    "        'selst': my_id\n",
    "    }),\n",
    "    ''\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can make a handy function for that\n",
    "def build_url(student_id):\n",
    "    return urlunparse([\n",
    "        'https',\n",
    "        'cacs.econ.msu.ru',\n",
    "        '/index.php',\n",
    "        '',\n",
    "        urlencode({\n",
    "            'mnu': '75',\n",
    "            'selst': student_id\n",
    "        }),\n",
    "        ''\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://cacs.econ.msu.ru/index.php?mnu=75&selst=20717\n",
      "https://cacs.econ.msu.ru/index.php?mnu=75&selst=20718\n",
      "https://cacs.econ.msu.ru/index.php?mnu=75&selst=20719\n"
     ]
    }
   ],
   "source": [
    "id_list = [20717, 20718, 20719]\n",
    "\n",
    "for student_id in id_list:\n",
    "    url = build_url(student_id)\n",
    "    print(url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.7",
   "language": "python",
   "name": "python3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
